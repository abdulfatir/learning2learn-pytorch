{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTSampler:\n",
    "    def __init__(self, train=True):\n",
    "        dataset = datasets.MNIST(\n",
    "            './mnist', train=train, download=True,\n",
    "            transform=torchvision.transforms.ToTensor())\n",
    "        self.loader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "    def sample(self):\n",
    "        while True:\n",
    "            for batch in self.loader:\n",
    "                yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, num_layers=1, num_units=20, sampler=None, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_layers = num_layers\n",
    "        if kwargs:\n",
    "            self.params = kwargs\n",
    "        else:\n",
    "            self.params = {}\n",
    "            in_size = 784\n",
    "            for i in range(num_layers):\n",
    "                self.params['linear_{:d}_W'.format(i)] =\\\n",
    "                    nn.Parameter(torch.randn(in_size, num_units) * 0.001)\n",
    "                self.params['linear_{:d}_b'.format(i)] =\\\n",
    "                    nn.Parameter(torch.zeros(1, num_units))\n",
    "                in_size = num_units\n",
    "            self.params['final_W'] =\\\n",
    "                nn.Parameter(torch.randn(in_size, 10) * 0.001)\n",
    "            self.params['final_b'] =\\\n",
    "                nn.Parameter(torch.zeros(1, 10))\n",
    "            self.mods = nn.ParameterList([v for v in self.params.values()])\n",
    "        self.act = nn.Sigmoid()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.sampler = sampler\n",
    "        if not sampler:\n",
    "            self.sampler = MNISTSampler().sample()\n",
    "    def _linear_forward(self, h, W, b, act=True):\n",
    "        h = torch.matmul(h, W) + b\n",
    "        if act:\n",
    "            h = self.act(h)\n",
    "        return h\n",
    "    def forward(self):\n",
    "        batch = next(self.sampler)\n",
    "        x, target = batch\n",
    "        h = x.view(x.size(0), -1)\n",
    "        for i in range(self.num_layers):\n",
    "            h = self._linear_forward(h, self.params['linear_{:d}_W'.format(i)], self.params['linear_{:d}_b'.format(i)])\n",
    "        y = self._linear_forward(h, self.params['final_W'], self.params['final_b'], act=False)\n",
    "        return self.loss(y, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=1e-2):\n",
    "        self.lr = lr\n",
    "    def __call__(self, grads):\n",
    "        updates = {}\n",
    "        for k, v in grads.items():\n",
    "            updates[k] = -self.lr * v\n",
    "        return updates\n",
    "class RMSProp:\n",
    "    def __init__(self, lr=1e-2, decay=0.99):\n",
    "        self.lr = lr\n",
    "        self.decay = decay\n",
    "        self.state = None\n",
    "    def __call__(self, grads):\n",
    "        if self.state is None:\n",
    "            self.state = {}\n",
    "            for k, v in grads.items():\n",
    "                self.state[k] = torch.zeros_like(v)\n",
    "        updates = {}\n",
    "        for k, v in grads.items():\n",
    "            state = self.state[k]\n",
    "            state = self.decay * state + (1 - self.decay) * torch.pow(v, 2)\n",
    "            self.state[k] = state\n",
    "            updates[k] = -self.lr * v / (torch.sqrt(state) + 1e-6)\n",
    "        return updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {'iter': [], 'loss': [], 'optim': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    mlp = MLP()\n",
    "    sgd = torch.optim.RMSprop(mlp.parameters(), lr=1e-3)\n",
    "    for i in range(100):\n",
    "        y = mlp()\n",
    "        losses['iter'].append(i)\n",
    "        losses['loss'].append(float(y.data.cpu().numpy()))\n",
    "        losses['optim'].append('SGD')\n",
    "        mlp.zero_grad()\n",
    "        y.backward()\n",
    "        sgd.step()\n",
    "    print(y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    mlp = MLP()\n",
    "    sgd = SGD()\n",
    "    for i in range(100):\n",
    "        y = mlp()\n",
    "        losses['iter'].append(i)\n",
    "        losses['loss'].append(float(y.data.cpu().numpy()))\n",
    "        losses['optim'].append('SGD')\n",
    "        mlp.zero_grad()\n",
    "        y.backward()\n",
    "        grads = {}\n",
    "        for k in mlp.params.keys():\n",
    "            grads[k] = mlp.params[k].grad.detach()\n",
    "        updates = sgd(grads)\n",
    "        for k in mlp.params.keys():\n",
    "            mlp.params[k].data += updates[k]\n",
    "    print(y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    mlp = MLP()\n",
    "    rms = RMSProp()\n",
    "    for i in range(100):\n",
    "        y = mlp()\n",
    "        losses['iter'].append(i)\n",
    "        losses['loss'].append(float(y.data.cpu().numpy()))\n",
    "        losses['optim'].append('RMSProp')\n",
    "        mlp.zero_grad()\n",
    "        y.backward()\n",
    "        grads = {}\n",
    "        for k in mlp.params.keys():\n",
    "            grads[k] = mlp.params[k].grad.detach()\n",
    "        updates = rms(grads)\n",
    "        for k in mlp.params.keys():\n",
    "            mlp.params[k].data += updates[k]\n",
    "    print(y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.lineplot(x='iter', y='loss', hue='optim', data=losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaOptimizer(nn.Module):\n",
    "    def __init__(self, dim=1, hidden=20, out=1, layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.layers = layers\n",
    "        self.lstm = nn.LSTM(input_size=dim, hidden_size=hidden, num_layers=layers)\n",
    "        self.linear = nn.Linear(hidden, out)\n",
    "    def forward(self, grad, state=None):\n",
    "        # seq_len x batch x size\n",
    "        grad = grad.view(1, -1, 1)\n",
    "        if state is None:\n",
    "            h = torch.zeros(self.layers, grad.size()[1], self.hidden)\n",
    "            c = torch.zeros(self.layers, grad.size()[1], self.hidden)\n",
    "            h = h.to(device)\n",
    "            c = c.to(device)\n",
    "            state = (h, c)\n",
    "        lstm_out, state = self.lstm(grad, state)\n",
    "        # lstm_out.shape: seq_len x batch x hidden\n",
    "        update = self.linear(lstm_out.view(-1, self.hidden))\n",
    "        return update, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "meta_optimizer = MetaOptimizer()\n",
    "meta_optimizer_optim = torch.optim.Adam(meta_optimizer.parameters(), lr = 1e-4)\n",
    "meta_optimizer = meta_optimizer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_mul = 0.1\n",
    "pbar = tqdm(range(500))\n",
    "state = None\n",
    "for k in pbar:\n",
    "    mlp = MLP()\n",
    "    states = {k: None for k in mlp.params.keys()}\n",
    "    for l in range(5):\n",
    "        loss = 0.\n",
    "        temp_params = {}\n",
    "        for i in range(20):\n",
    "            y = mlp()\n",
    "            loss += y\n",
    "            mlp.zero_grad()\n",
    "            y.backward(retain_graph=True)\n",
    "            grads = {}\n",
    "            for k in mlp.params.keys():\n",
    "                grads[k] = mlp.params[k].grad.detach()\n",
    "            updates = {}\n",
    "            for k in grads.keys():\n",
    "                updates[k], states[k] = meta_optimizer(grads[k], states[k])\n",
    "            for k in updates.keys():\n",
    "                temp_params[k] = mlp.params[k] + updates[k].view_as(mlp.params[k]) * out_mul\n",
    "                temp_params[k].retain_grad()\n",
    "            mlp = MLP(**temp_params)\n",
    "        for k in states.keys():\n",
    "            states[k] = (states[k][0].detach().clone(), states[k][1].detach().clone())\n",
    "        meta_optimizer_optim.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        meta_optimizer_optim.step()\n",
    "    pbar.set_description('Loss: %.3f' % loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_optimizer.eval()\n",
    "for _ in range(10):\n",
    "    q = QuadraticModel()\n",
    "    state = None\n",
    "    for i in range(100):\n",
    "        y = q()\n",
    "        losses['iter'].append(i)\n",
    "        losses['loss'].append(float(y.data.cpu().numpy()))\n",
    "        losses['optim'].append('L2L')\n",
    "        grads = torch.autograd.grad(y, q.theta)[0].detach()\n",
    "        update, state = meta_optimizer(grads, state)\n",
    "        update = update.view_as(q.theta.data)\n",
    "        q.theta.data += update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = seaborn.lineplot(x='iter', y='loss', hue='optim', data=losses)\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(meta_optimizer.state_dict(), 'meta-qf.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.optim == 'L2L'].groupby('iter').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.optim == 'RMSProp'].groupby('iter').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
