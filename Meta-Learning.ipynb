{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Learn by Gradient Descent by Gradient Descent\n",
    "## As simple as possible in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I first came across DeepMind’s paper “Learning to learn by gradient descent by gradient descent”, my reaction was “Wow, how on earth does that work?”. Unfortunately, my first read-through of the paper was not that illuminating, and looking at the code was quite daunitng.\n",
    "Thankfully, I was intrigued enough to force myself to re-read the paper and it actually turned out to be surprisingly simple in the end. Personally, what really helps me when I’m trying to understand something is to create the simplest non-trivial version of the problem and then scale up from there. So here it is, the simplest version of the idea I could create, also no maths equations! I hope you find it illuminating.\n",
    "I suggest skimming the paper first but this should all be understandable without. Code was ran on TF v1.0.0-rc1.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "So let’s get started. Time to learn about learning to learn by gradient descent by gradient descent by reading my article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"b'v1.13.0-rc2-5-g6612da8'\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.GIT_VERSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we need a problem for our meta-learning optimizer to solve. Let’s take the simplest experiment from the paper; finding the minimum of a multi-dimensional quadratic function. We are going to randomly scale the parabolas and start at random locations, the solution is always at (0, 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMS = 10  # Dimensions of the parabola\n",
    "scale = tf.random_uniform([DIMS], 0.5, 1.5)\n",
    "# This represents the network we are trying to optimize,\n",
    "# the `optimizee' as it's called in the paper.\n",
    "# Actually, it's more accurate to think of this as the error\n",
    "# landscape.\n",
    "def f(x):\n",
    "    x = scale*x\n",
    "    return tf.reduce_sum(x*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can’t easily use TensorFlow’s built in optimizers here since the technique requires us to unroll the training loop inside the computation graph, as we’ll see in a bit. So let’s define a couple of simple hand crafted optimizers to test against our learned optimizer. As discussed in the paper, an optimizer is a function g that takes the gradient of a parameter at a given step and returns back the step you should take in parameter space. Here’s vanilla gradient descent: (Some optimizers need to keep track of state, here I just pass the param through)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_sgd(gradients, state, learning_rate=0.1):\n",
    "    return -learning_rate*gradients, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a stronger baseline let’s use RMSProp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g_rms(gradients, state, learning_rate=0.1, decay_rate=0.99):\n",
    "    if state is None:\n",
    "        state = tf.zeros(DIMS)\n",
    "    state = decay_rate*state + (1-decay_rate)*tf.pow(gradients, 2)\n",
    "    update = -learning_rate*gradients / (tf.sqrt(state)+1e-5)\n",
    "    return update, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let’s unroll all the training steps, here learn is a function which takes one of these optimizers and applies it in a loop for number of steps and collects the value of the function f at each point, which we can think of as our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_STEPS = 20  # This is 100 in the paper\n",
    "\n",
    "initial_pos = tf.random_uniform([DIMS], -1., 1.)\n",
    "\n",
    "def learn(optimizer):\n",
    "    losses = []\n",
    "    x = initial_pos\n",
    "    state = None\n",
    "    for _ in range(TRAINING_STEPS):\n",
    "        loss = f(x)\n",
    "        losses.append(loss)\n",
    "        grads, = tf.gradients(loss, x)\n",
    "      \n",
    "        update, state = optimizer(grads, state)\n",
    "        x += update\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let’s test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_losses = learn(g_sgd)\n",
    "rms_losses = learn(g_rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see what the losses look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6524445  1.320545   0.74690986 0.8601543  1.4829196  0.8428761\n",
      " 0.8197861  0.7435862  1.4918832  1.0263679 ]\n",
      "[1.4862759  0.5460484  1.4281286  0.6151525  1.3203219  1.0649748\n",
      " 1.2313627  1.3046669  0.5159708  0.84501505]\n",
      "[0.6417544  1.0662564  0.75994337 1.381727   1.2855614  1.427719\n",
      " 0.8364997  0.5813335  1.4972899  0.5615797 ]\n",
      "[0.6480937  1.0246218  0.9616436  1.3109506  1.4934438  1.0432948\n",
      " 0.818835   1.2945511  0.96391416 1.3681219 ]\n",
      "[0.82109654 1.4743366  1.4300326  1.0077832  1.0782108  0.8298131\n",
      " 1.1598417  1.4137295  1.4398162  1.2656555 ]\n",
      "[1.4952469  1.41593    1.3975434  1.2364196  0.7607646  1.0676229\n",
      " 0.9370997  0.75032127 1.1599799  0.9686446 ]\n",
      "[0.7090212 1.1732113 1.4039859 1.0987186 1.3852904 1.0666554 1.1765866\n",
      " 0.7300105 1.065237  1.4985209]\n",
      "[0.8547069  0.84800375 1.0561929  0.5405568  0.98526895 0.76719534\n",
      " 0.7709173  0.8096771  0.6647085  0.5633023 ]\n",
      "[1.0806595  0.7398472  0.94033074 0.6657038  1.3854527  0.57082057\n",
      " 1.1130118  1.3288243  0.91984606 1.3385088 ]\n",
      "[0.864889   0.78945637 0.9547951  1.0504059  0.6955073  1.1734033\n",
      " 1.1134231  0.8237078  1.2955709  1.0494345 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "x = np.arange(TRAINING_STEPS)\n",
    "sgd_l = np.zeros(TRAINING_STEPS)\n",
    "rms_l = np.zeros(TRAINING_STEPS)\n",
    "TRIALS = 10\n",
    "for _ in range(TRIALS): \n",
    "    sgd_np, rms_np, ipos, scale_np = sess.run([sgd_losses, rms_losses, initial_pos, scale])\n",
    "    print(scale_np)\n",
    "    sgd_l += sgd_np\n",
    "    rms_l += rms_np\n",
    "sgd_l /= TRIALS\n",
    "rms_l /= TRIALS\n",
    "p1, = plt.plot(x, sgd_l, label='SGD')\n",
    "p2, = plt.plot(x, rms_l, label='RMS')\n",
    "plt.legend(handles=[p1, p2])\n",
    "plt.title('Losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Meta-Learning\n",
    "Time to put together our meta-learning optimizer, just like in the paper this is an LSTM with 2 layers and 20 hidden units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-10-42878c2c1046>:5: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-10-42878c2c1046>:5: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    }
   ],
   "source": [
    "LAYERS = 2\n",
    "STATE_SIZE = 20\n",
    "\n",
    "cell = tf.contrib.rnn.MultiRNNCell(\n",
    "    [tf.contrib.rnn.LSTMCell(STATE_SIZE) for _ in range(LAYERS)])\n",
    "cell = tf.contrib.rnn.InputProjectionWrapper(cell, STATE_SIZE)\n",
    "cell = tf.contrib.rnn.OutputProjectionWrapper(cell, 1)\n",
    "cell = tf.make_template('cell', cell)\n",
    "\n",
    "def g_rnn(gradients, state):\n",
    "    # Make a `batch' of single gradients to create a \n",
    "    # \"coordinate-wise\" RNN as the paper describes. \n",
    "    gradients = tf.expand_dims(gradients, axis=1)\n",
    "    if state is None:\n",
    "        state = [[tf.zeros([DIMS, STATE_SIZE])] * 2] * LAYERS\n",
    "#     print(len(state), len(state[0]), state[0][0].name, state[0][1].name)\n",
    "    update, state = cell(gradients, state)\n",
    "    # Squeeze to make it a single batch again.\n",
    "    return tf.squeeze(update, axis=[1]), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And *that’s it*, that’s our meta learner. We can use it in exactly the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/abdulfatir/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "rnn_losses = learn(g_rnn)\n",
    "sum_losses = tf.reduce_sum(rnn_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here’s the magic bit, we want sum_losses to be low, since the lower the losses the better the optimizer right? And because the entire training loop is in the graph we can use Back-Propagation Through Time (BPTT) and a meta-optimizer to minimize this value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the main point: *sum_losses is differentiable*, gradients flow through the graph we’ve defined just fine! TensorFlow is able to work out the gradients of the parameters in our LSTM with respect to this sum of losses. So let’s optimize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.0001)\n",
    "def optimize(loss):\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.)\n",
    "    return optimizer.apply_gradients(zip(gradients, v))\n",
    "\n",
    "apply_update = optimize(sum_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I found gradient clipping to be *critical* here since the values that go into our meta-learner can be very large at the start of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426.2877197265625\n",
      "18.126384924411774\n",
      "4.236409852266312\n",
      "3.986720544934273\n",
      "3.958878756582737\n",
      "CPU times: user 1min 44s, sys: 17.8 s, total: 2min 2s\n",
      "Wall time: 5min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sess.run(tf.global_variables_initializer())\n",
    "ave = 0\n",
    "for i in range(4000):\n",
    "    err, _ = sess.run([sum_losses, apply_update])\n",
    "    ave += err\n",
    "    if i % 1000 == 0:\n",
    "        print(ave / 1000 if i!=0 else ave)\n",
    "        ave = 0\n",
    "print(ave / 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And see how it does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(3): \n",
    "    sgd_l, rms_l, rnn_l = sess.run(\n",
    "        [sgd_losses, rms_losses, rnn_losses])\n",
    "    p1, = plt.plot(x, sgd_l, label='SGD')\n",
    "    p2, = plt.plot(x, rms_l, label='RMS')\n",
    "    p3, = plt.plot(x, rnn_l, label='RNN')\n",
    "    plt.legend(handles=[p1, p2, p3])\n",
    "    plt.title('Losses')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Looks like it’s doing even better than RMS on this problem. Actually, these graphs are slightly misleading, log scale shows something slightly different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for _ in range(3): \n",
    "    sgd_l, rms_l, rnn_l = sess.run(\n",
    "        [sgd_losses, rms_losses, rnn_losses])\n",
    "    p1, = plt.semilogy(x, sgd_l, label='SGD')\n",
    "    p2, = plt.semilogy(x, rms_l, label='RMS')\n",
    "    p3, = plt.semilogy(x, rnn_l, label='RNN')\n",
    "    plt.legend(handles=[p1, p2, p3])\n",
    "    plt.title('Losses')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think the reason for this, again as discussed in the paper, is that the magnitude of the values being fed into the LSTM can vary wildly and neural networks generally do not perform well when that happens. Here the gradients get so small that it isn’t able to compute sensible updates. The paper uses a solution to this for the bigger experiments; feed in the log gradient and the direction instead. See the paper for details.\n",
    "Hopefully, now that you understand how learn to learn by gradient descent by gradient descent you can see the limitations. It doesn’t seem very scalable. I think it is quite telling that the experiments in the paper are very small. It takes 4000 steps for even our toy problem to converge, we had to train a network completely just for one step of optimization for the meta-learner. We would have to optimize a large problem many more times and would take a very long time. Also unrolling the entire training loop in the graph is not feasible for larger problems, although in the paper they only unroll the BPTT to 20 steps. There is also evidence in the paper that the RNN optimizer can generalize from smaller problem to larger problems.\n",
    "I expect meta-learning to become increasingly more important, for more inspiration I suggest watching these NIPS presentations. https://www.youtube.com/watch?v=0YLpp_TVhLY&list=PLPwzH56Rdmq4hcuEMtvBGxUrcQ4cAkoSc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm =nn.LSTM(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10\n",
    "num_classes = 50\n",
    "batch_size = 64\n",
    "state_size = 100\n",
    "x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "embeddings = tf.get_variable('embedding_matrix', [num_classes, state_size])\n",
    "print(x, embeddings)\n",
    "tf.nn.embedding_lookup(embeddings, x)\n",
    "rnn_inputs = [tf.squeeze(i) for i in tf.split(tf.nn.embedding_lookup(embeddings, x), num_steps, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
