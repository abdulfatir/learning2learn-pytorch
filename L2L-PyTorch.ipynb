{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuadraticModel(nn.Module):\n",
    "    def __init__(self, dims=10, scale=None, theta=None, z=None):\n",
    "        super().__init__()\n",
    "        if theta is not None and scale is not None:\n",
    "            self.scale = scale\n",
    "            self.theta = theta\n",
    "            self.z = z\n",
    "        else:\n",
    "            scale = torch.tensor(np.random.uniform(low=0.5, high=1.5, size=(dims, dims)).astype(np.float32))\n",
    "            theta = torch.tensor(np.random.uniform(low=-1, high=1, size=(dims, 1)).astype(np.float32))\n",
    "            z = torch.tensor(np.random.uniform(low=-1, high=1, size=(dims, 1)).astype(np.float32))\n",
    "            self.z = z.to(device)\n",
    "            self.scale = scale.to(device)\n",
    "            theta = theta.to(device)\n",
    "            self.theta = nn.Parameter(theta)\n",
    "    def forward(self):\n",
    "        o = torch.matmul(self.scale, self.theta) - self.z\n",
    "        o.squeeze_()\n",
    "        return torch.sum(o ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, lr=1e-3):\n",
    "        self.lr = lr\n",
    "    def __call__(self, grads):\n",
    "        return -self.lr * grads\n",
    "class RMSProp:\n",
    "    def __init__(self, lr=1e-2, decay=0.99):\n",
    "        self.lr = lr\n",
    "        self.decay = decay\n",
    "        self.state = None\n",
    "    def __call__(self, grads):\n",
    "        if self.state is None:\n",
    "            self.state = torch.zeros_like(grads)\n",
    "        state = self.state\n",
    "        state = self.decay * state + (1 - self.decay) * torch.pow(grads, 2)\n",
    "        self.state = state\n",
    "        update = -self.lr * grads / (torch.sqrt(state) + 1e-6)\n",
    "        return update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = {'iter': [], 'loss': [], 'optim': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    q = QuadraticModel()\n",
    "    sgd = SGD()\n",
    "    for i in range(20):\n",
    "        y = q()\n",
    "        losses['iter'].append(i)\n",
    "        losses['loss'].append(float(y.data.cpu().numpy()))\n",
    "        losses['optim'].append('SGD')\n",
    "        grads = torch.autograd.grad(y, q.theta)[0].detach()\n",
    "        update = sgd(grads)\n",
    "        q.theta.data += update\n",
    "    print(y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    q = QuadraticModel()\n",
    "    rms = RMSProp()\n",
    "    for i in range(20):\n",
    "        y = q()\n",
    "        losses['iter'].append(i)\n",
    "        losses['loss'].append(float(y.data.cpu().numpy()))\n",
    "        losses['optim'].append('RMSProp')\n",
    "        grads = torch.autograd.grad(y, q.theta)[0].detach()\n",
    "        update = rms(grads)\n",
    "        q.theta.data += update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.lineplot(x='iter', y='loss', hue='optim', data=losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaOptimizer(nn.Module):\n",
    "    def __init__(self, dim=1, hidden=20, out=1, layers=2):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.layers = layers\n",
    "        self.lstm = nn.LSTM(input_size=dim, hidden_size=hidden, num_layers=layers)\n",
    "        self.linear = nn.Linear(hidden, out)\n",
    "    def forward(self, grad, state=None):\n",
    "        # seq_len x batch x size\n",
    "        grad = grad.view(1, -1, 1)\n",
    "        if state is None:\n",
    "            h = torch.zeros(self.layers, grad.size()[1], self.hidden)\n",
    "            c = torch.zeros(self.layers, grad.size()[1], self.hidden)\n",
    "            h = h.to(device)\n",
    "            c = c.to(device)\n",
    "            state = (h, c)\n",
    "        lstm_out, state = self.lstm(grad, state)\n",
    "        # lstm_out.shape: seq_len x batch x hidden\n",
    "        update = self.linear(lstm_out.view(-1, self.hidden))\n",
    "        return update, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "meta_optimizer = MetaOptimizer()\n",
    "meta_optimizer_optim = torch.optim.Adam(meta_optimizer.parameters(), lr = 1e-4)\n",
    "meta_optimizer = meta_optimizer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pbar = tqdm(range(2000))\n",
    "state = None\n",
    "for k in pbar:\n",
    "    q = QuadraticModel()\n",
    "    state = None\n",
    "    for l in range(5):\n",
    "        loss = 0.\n",
    "        temp_theta = 0.\n",
    "        for i in range(20):\n",
    "            y = q()\n",
    "            loss += y\n",
    "            if q.theta.grad is not None:\n",
    "                q.theta.grad.zero_()\n",
    "            y.backward(retain_graph=True)\n",
    "            grads = q.theta.grad.detach()\n",
    "            update, state = meta_optimizer(grads, state)\n",
    "            update = update.view_as(q.theta.data)\n",
    "            temp_theta = q.theta + update\n",
    "            temp_theta.retain_grad()\n",
    "            q = QuadraticModel(scale=q.scale, theta=temp_theta, z=q.z)\n",
    "        state = (state[0].detach().clone(), state[1].detach().clone())\n",
    "        meta_optimizer_optim.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        nn.utils.clip_grad_norm_(meta_optimizer.parameters(), 1.)\n",
    "        meta_optimizer_optim.step()\n",
    "    pbar.set_description('Loss: %.3f' % loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_optimizer.eval()\n",
    "for _ in range(10):\n",
    "    q = QuadraticModel()\n",
    "    state = None\n",
    "    for i in range(20):\n",
    "        y = q()\n",
    "        losses['iter'].append(i)\n",
    "        losses['loss'].append(float(y.data.cpu().numpy()))\n",
    "        losses['optim'].append('L2L')\n",
    "        grads = torch.autograd.grad(y, q.theta)[0].detach()\n",
    "        update, state = meta_optimizer(grads, state)\n",
    "        update = update.view_as(q.theta.data)\n",
    "        q.theta.data += update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seaborn.lineplot(x='iter', y='loss', hue='optim', data=losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.optim == 'L2L'].groupby('iter').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.optim == 'RMSProp'].groupby('iter').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
